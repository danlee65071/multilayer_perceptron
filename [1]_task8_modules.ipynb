{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIDgsmtTLT89"
      },
      "source": [
        "См. сначала **`[0]_task8_train_model.ipynb`**, в котором описана структура этого задания.  \n",
        "\n",
        "После выполнения этого ноутбука см. **`[2]_task8_test_modules.ipynb`** для проверки работы всех компонент."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQjtx_23LT9E"
      },
      "source": [
        "## Мотивация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8NbmAcwLT9F"
      },
      "source": [
        "<img src=\"https://i.pinimg.com/originals/2f/49/84/2f4984329848be3825c17672beef797e.png\" width=550>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KAXzni-LT9G"
      },
      "source": [
        "Мы хотим построить \"с нуля\" свой мини-фреймворк для обучения нейронных сетей. Он должен позволять создавать, обучать и тестировать нейросети. Как известно из лекции и семинара, **цикл обучения нейросети** выглядит так:\n",
        "\n",
        "```\n",
        "# однослойная нейросеть \n",
        "model = Sequential()\n",
        "model.add(Linear(2,2))\n",
        "model.add(LogSoftMax())\n",
        "\n",
        "criterion = NLLCriterion()\n",
        "\n",
        "optimizer = SGD(lr=1e-2, momentum=0.9)\n",
        "\n",
        "# одна эпоха -- один проход по обучающей выборке\n",
        "for i in range(n_epoch):\n",
        "    # одна итерация -- один батч\n",
        "    for x_batch, y_batch in train_generator(sample, labels, batch_size):\n",
        "        # Обнуляем градиенты с предыдущей итерации\n",
        "        model.zero_grad_params()\n",
        "        # Forward pass\n",
        "        predictions = model.forward(x_batch)\n",
        "        loss = criterion.forward(predictions, y_batch)\n",
        "        # Backward pass\n",
        "        last_grad_input = criterion.backward(predictions, y_batch)\n",
        "        model.backward(x_batch, last_grad_input)\n",
        "        # Обновление весов\n",
        "        optimizer(\n",
        "            model.get_params(), \n",
        "            model.get_grad_params(), \n",
        "            opt_params,\n",
        "            opt_state\n",
        "        )\n",
        " ```\n",
        " \n",
        "Одна итерация внутреннего цикла называется **одной итерацией обучения нейросети**. Одна итерация внешнего цикла называется **одной эпохой обучения нейросети**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMqw-W-6LT9H"
      },
      "source": [
        "## Проектирование фреймворка "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS4c3RDgLT9H"
      },
      "source": [
        "### Базовые концепции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVT7CxlfLT9I"
      },
      "source": [
        "**Нейросеть** $-$ это последовательность слоев. В реализации ее удобно представлять абстракцией `Sequential`. \n",
        "\n",
        "**Слой** $-$ это некоторая функция, у которой в общем случае есть обучаемые параметры. Есть слои и без обучаемых параметров (например, функции активации, SoftMax, LogSoftMax), однако все эти функции все равно удобно называть слоями нейросети. В реализации один слой удобно представлять абстракцией `Module`.  Например, `Sequential(Linear, ReLU)` -- это три уже модуля.\n",
        "\n",
        "Каждый слой должен уметь делать прямой проход **forward pass**, и обратный проход **backward pass**. В реализации forward pass удобно представлять абстрактным методом `forward()`, backward pass удобно представлять абстрактным методом `backward()`.\n",
        "\n",
        "<img src=\"https://i.ibb.co/fFQtC15/2020-04-14-16-42-23.png\" width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J8JcoHtLT9J"
      },
      "source": [
        "### Forward pass\n",
        "\n",
        "Forward pass является *первым этапом итерации обучения нейросети*. После выполнения этого этапа сеть должна выдать вычисленное преобразование входа.\n",
        "\n",
        "Во время вызова метода `forward()` у `Sequential`, вход, поданный нейросети, проходит через все ее слои \"вперед\", до выходного слоя.\n",
        "\n",
        "Во время вызова метода `forward()` у `Module`, над входом, поданным слою, осуществляется операция этого слоя (линейная, дропаут, софтмакс, батчнорм).\n",
        "\n",
        "В реализации ниже у каждого слоя во время `forward()` будет вызываться только один метод $-$ `update_output()`, который и производит вычисление операции слоя. Важно отметить, что при вызове `update_output()` его выход **сохраняется в поле `self.output`** вызвавшего слоя. Это необходимо, поскольку выходы слоёв потом используются в **backward pass**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4Z0Qyp-LT9L"
      },
      "source": [
        "### Backward pass\n",
        "\n",
        "#### Теоретическая справка\n",
        "\n",
        "Backward pass является *вторым этапом итерации обучения нейросети*. В современном глубоком обучении backward pass является реализацией метода **[Error Backpropagation](https://en.wikipedia.org/wiki/Backpropagation)** (backprop), по-русски **\"Метод обратного распространения ошибки\"**. После выполнения этого этапа у каждого параметра каждого слоя нейронной сети должны быть посчитаны градиенты на текущей итерации. \n",
        "\n",
        "**Первая идея** Backpropagation состоит в использования **градиентных методов оптимизации**, например, стохастического градиентного спуска. Однако чтобы посчитать градиент функции потерь $L$ по параметрам ранних слоев в нейросети, придется иметь дело с \"очень сложной\" функцией:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_1} = \\frac{\\partial (\\varphi_n(W_n\\varphi_{n-1}(W_{n-1}\\varphi_{n-2}(...\\varphi_{1}(W_1 x)))) - y)^2}{\\partial W_1}\n",
        "$$\n",
        "\n",
        "где $W_k$ $-$ матрица весов $k$-го слоя, $\\varphi_k$ $-$ функция активации $k$-го слоя. И это при том, что здесь все слои $-$ линейные. Для более сложных слоев (например, свёрточных) эта функция будет еще сложнее.  \n",
        "\n",
        "Поэтому **вторая идея** Backpropagation состоит в использовании **правила цепочки (chain rule)**, примененного в отношении градиента функции потерь по каждому из параметров каждого слоя нейросети. Рассмотрим конкретный слой под номером $k$ и следующим за ним слой $k+1$. Пусть это оба $-$ линейные слои, линейный слой $k$ осуществляет операцию $O_k = x_k W_k$, где $x_k \\in \\mathbb{R}^{n \\times d}$ $-$ вход слоя, $W_k \\in \\mathbb{R}^{d \\times m}$ $-$ веса слоя, $O_k \\in \\mathbb{R}^{n \\times m}$ $-$ выход слоя. Тогда чтобы обновить его веса $W_k$ выпишем правило цепочки:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial W_k} = \\frac{\\partial L}{\\partial O_k} \\frac{\\partial O_k}{\\partial W_k} = \\frac{\\partial L}{\\partial O_k} x_k\n",
        "$$\n",
        "\n",
        "Видим, что для вычисления градиента лосса по $W_k$ нам нужно посчитать $\\frac{\\partial L}{\\partial O_k}$, то есть градиент лосса по выходу этого слоя. **Если слой $k$ является последним (выходным)** в нейросети (то есть $k+1$-го слоя уже нет), то ответ имеет вид:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial O_k} = \\frac{\\partial L}{\\partial \\widehat{y}_k}\n",
        "$$\n",
        "\n",
        "И сразу же достигаем цели. Однако **если слой $k$ $-$ это какой-то из скрытых слоев**, то мы не можем сразу посчитать $\\frac{\\partial L}{\\partial O_k}$ $-$ придем к той же проблеме \"очень сложной\" функции, описанной выше.  \n",
        "\n",
        "<img src=\"https://i.ibb.co/2hn6wb3/2020-04-14-17-19-29.png\" width=700>\n",
        "\n",
        "Поэтому чтобы реализовать правило цепочки, делается такой \"трюк\": заметим, что **выход слоя $k$ является входом для слоя $k+1$**, то есть $O_k = x_{k+1}$. И тогда будем для каждого слоя считать не только $\\frac{\\partial L}{\\partial W_{k+1}}$ для обновления весов, но и $\\frac{\\partial L}{\\partial x_{k+1}}$ для передачи градиента по входу слоя $k+1$ в виде градиента по выходу $\\frac{\\partial L}{\\partial O_k}$ слоя $k$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_{k+1}} = \\frac{\\partial L}{\\partial O_{k+1}} \\frac{\\partial O_{k+1}}{\\partial x_{k+1}} = \\frac{\\partial L}{\\partial O_{k+1}} W_{k+1}   \n",
        "$$\n",
        "\n",
        "Делая так для **каждого** слоя, мы получим возможность как бы **рекурсивно** обновлять параметры (веса) всех слоев, как только получим $\\frac{\\partial L}{\\partial \\hat{y_k}}$ от последнего слоя.\n",
        "\n",
        "#### Реализация\n",
        "\n",
        "Во время вызова метода `backward()` у `Sequential` мы в цикле вычисляем `backward()` для всех слоев нейросети в соответствии с описанной выше схемой реализации правила цепочки.\n",
        "\n",
        "Во время вызова метода `backward()` у `Module` вызываются два метода $-$ `update_grad_params()` и `update_grad_input()`.\n",
        "\n",
        "`update_grad_params()` вычисляет $\\frac{\\partial O_k}{\\partial W_k}$ $-$ градиент выхода слоя по параметрам $W_k$.\n",
        "\n",
        "`update_grad_input()` вычисляет $\\frac{\\partial O_k}{\\partial x_k}$ $-$ градиент выхода слоя по входу $x_k$, чтобы передать потом этот градент слою $k-1$ в виде `grad_output`.\n",
        "\n",
        "***Важно:*** в chain rule присутствуют произведения градиентов. Они могут быть векторами/матрицами, поэтому при умножении следует использовать именно **матричное произведение**, если выводите формулы через прозводную по вектору/матрице. Если же выводите \"поэлементно\" (как в примере с `LogSoftMax`), то форма произведений будет видна из вывода."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwyYLB9ILT9N"
      },
      "source": [
        "Обратите внимание на то, что в цикле обучения выше (под картинкой в начале раздела \"Мотивация\") `last_grad_input` $-$ это градиент слоя `criterion` по его входу, и он же является `grad_output` для всей нейросети `model` $-$ градиентом, приходящим от \"следующего слоя\". Это полностью согласуется с методом обратного распространения ошибки, который мы только что обсудили, если считать функцию потерь (`criterion`) \"фиктивным\" слоем нейросети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39UTTy1oLT9N"
      },
      "source": [
        "*Примечание*: вообще говоря, сам метод обновления весов нейросети не обязан быть gradient-based, каким является backprop. Например, это могут быть [эволюционные алгоритмы](https://arxiv.org/pdf/1712.06567.pdf), или относительно недавний Equilibrium propagation, см. [ответ на StackOverflow](https://stackoverflow.com/questions/55287004/are-there-alternatives-to-backpropagation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2E-JGfRLT9O"
      },
      "source": [
        "### Обновление весов\n",
        "\n",
        "Обновление весов (оптимизация) является *третьим, последним этапом итерации обучения нейросети*. После выполнения этого этапа все обучаемые параметры всех слоев нейросети должны изменить свое значение (обновиться) в соответствие с правилами данного конкретного оптимизатора.\n",
        "\n",
        "В реализации ниже вам нужно написать только один оптимизатор $-$ `SGD`. Это метод стохастического градиентного спуска."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3llAxwxLT9O"
      },
      "source": [
        "## Реализация (10 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edyuzHU7LT9O"
      },
      "source": [
        "<img src=\"https://i.insider.com/5cdc1519021b4c73922760ab?width=1100&format=jpeg&auto=webp\" width=550>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR6CbroLLT9P"
      },
      "source": [
        "Далее вам предстоит реализовать все компоненты нейронной сети, используя **только библиотеку NumPy**:\n",
        "\n",
        "> Базовые концепции:\n",
        "- [x] `Module`     $-$ абстрактный класс для компонент нейронной сети;\n",
        "- [ ] *(2 балла)* `Sequential` $-$ класс, содержащий в себе последовательность объектов класса `Module`.\n",
        "\n",
        "> Слои:\n",
        "- [ ] *(2 балла)* `Linear`     $-$ линейный слой;\n",
        "- [ ] *(2 балла)* `SoftMax`    $-$ слой, вычисляющий операцию *softmax*;\n",
        "- [x] `LogSoftMax` $-$ слой, вычисляющий операцию *log(softmax)*;\n",
        "\n",
        "> Функции активации (тоже являются слоями, но выделены в отдельную секцию для удобства):\n",
        "- [ ] *(1 балл)* `ReLU`      $-$ функция активации *Rectified Linear Unit*;\n",
        "\n",
        "> Функции потерь:\n",
        "- [x] `Criterion`  $-$ абстрактный класс для функций потерь;\n",
        "- [ ] *(1 балл)* `NLLCriterionUnstable` $-$ negative log-likelihood функция потерь (нестабильная версия, возможны числовые переполнения);\n",
        "- [x] *(1 балл)* `NLLCriterion` $-$ negative log-likelihood функция потерь (стабильная версия).\n",
        "\n",
        "> Оптимизаторы:\n",
        "- [ ] *(1 балл)* `SGD`  $-$ алгоритм стохастического градиентного спуска.\n",
        "\n",
        "Перед каждым слоем напоминается формула его forward pass. В уже реализованных за вас модулях (отмечены галочкой) формулы для вычисления backward pass тоже уже даны, в остальных их нужно вывести самим по аналогии.\n",
        "\n",
        "**В скобках перед названием слоя указаны баллы за его реализацию и за вывод формулы для backward pass. Они засчитываются только тогда, когда слой проходит все тесты в ноутбуке `[2]task8_test_modules.ipynb`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9zljPBjLT9Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twkdR5KsLT9R"
      },
      "source": [
        "##  Базовые концепции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe2EzNqBLT9S"
      },
      "source": [
        "### Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4ee2hw1LT9S"
      },
      "source": [
        "**Module** $-$ абстрактный класс, который определяет методы, которые могут быть реализованы у каждого слоя.\n",
        "\n",
        "Этот класс полностью реализован за вас. Пожалуйста, внимательно прочитайте методы и их описания, чтобы ориентироваться в дальнейшем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3smvdgDHLT9S"
      },
      "outputs": [],
      "source": [
        "class Module(object):\n",
        "    \"\"\"\n",
        "        Абстрактный класс для слоев нейросети.\n",
        "\n",
        "        Как и описано в \"Проектирование фреймворка\":\n",
        "\n",
        "        - во время forward просто вычисляет операцию слоя:\n",
        "\n",
        "            `output = module.forward(input)`\n",
        "\n",
        "        - во время backward дифференцирует функцию слоя по входу и по параметрам,\n",
        "          возвращает градиент по входу этого слоя (для удобства):\n",
        "\n",
        "            `grad_input = module.backward(input, grad_output)`\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.grad_input = None\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Вычисляет операцию слоя.\n",
        "        \n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя  \n",
        "        Выход: \n",
        "            `self.update_output(input) (np.array)` -- вычисленная операция слоя\n",
        "        \"\"\"\n",
        "        \n",
        "        return self.update_output(input)\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Осуществляет шаг backpropagation'а для этого слоя,\n",
        "        дифференцируя функцию слоя по входу и по параметрам.\n",
        "        \n",
        "        Обратите внимание, что градиент зависит и от параметров, от входа input.\n",
        "        \n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        Выход: \n",
        "            `self.grad_input (np.array)` -- градиент функции слоя по входу\n",
        "        \"\"\"\n",
        "        \n",
        "        self.update_grad_input(input, grad_output)\n",
        "        self.update_grad_params(input, grad_output)\n",
        "        return self.grad_input\n",
        "    \n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Конкретная реализация `forward()` для данного слоя.\n",
        "        Вычисляет функцию слоя (линейную, `ReLU`, `SoftMax`) по входу `input`.\n",
        "        \n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя\n",
        "        Выход: \n",
        "            `self.output (np.array)` -- вычисленная операция слоя, сохраненная в поле класса \n",
        "            \n",
        "        Важно! не забывайте как возвращать `self.output`, так и сохранять результат в это поле \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "            \n",
        "        # self.output = input \n",
        "        # return self.output\n",
        "        \n",
        "        pass\n",
        "\n",
        "    def update_grad_input(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вычисляет градиент функции слоя по входу `input` и возвращает его в виде `self.grad_input`.\n",
        "        Размер (`shape`) поля `self.grad_input` всегда совпадает с размером `input`.\n",
        "        \n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        Выход: \n",
        "            `self.grad_input (np.array)` -- вычисленный градиент функции слоя по входу `input`\n",
        "        \n",
        "        Важно! не забывайте как возвращать `self.grad_input`, так и сохранять результат в это поле \n",
        "        \"\"\"\n",
        "        \n",
        "        # The easiest case:\n",
        "        \n",
        "        # self.grad_input = grad_output \n",
        "        # return self.grad_input\n",
        "        \n",
        "        pass   \n",
        "    \n",
        "    def update_grad_params(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вычисляет градиент функции слоя по параметрам (весам) этого слоя. \n",
        "        Ничего не возвращает, только сохраняет значения градиентов в соответствующие поля.\n",
        "        Не нужно реализовывать этот метод, если у слоя нет параметров (у функций активации, \n",
        "        `SoftMax`, `LogSoftMax`, `MaxPool2d`).\n",
        "        \n",
        "        Вход: \n",
        "           `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def zero_grad_params(self): \n",
        "        \"\"\"\n",
        "        Обнуляет градиенты у параметров слоя (если они есть).\n",
        "        Нужно для оптимизатора.\n",
        "        \"\"\"\n",
        "        \n",
        "        pass\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Возвращает список параметров этого слоя, если они есть. Иначе вернуть пустой список. \n",
        "        Нужно для оптимизатора.\n",
        "        \"\"\"\n",
        "        \n",
        "        return []\n",
        "        \n",
        "        \n",
        "    def get_grad_params(self):\n",
        "        \"\"\"\n",
        "        Возвращает список градиентов функции этого слоя по параметрам этого слоя, если они есть. \n",
        "        Иначе вернуть пустой список. \n",
        "        Нужно для оптимизатора.\n",
        "        \"\"\"\n",
        "        \n",
        "        return []\n",
        "    \n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Напечатать название слоя КРАСИВО.\n",
        "        \"\"\"\n",
        "        \n",
        "        return 'Module'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dTkEMXhLT9U"
      },
      "source": [
        "### Sequential (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pttxoLWFLT9U"
      },
      "source": [
        "Многослойная нейронная сеть состоит из последовательности модулей. Реализуйте класс **Sequential**, руководствуюясь механикой forward и backward pass'ов и описаниями каждого метода.\n",
        "\n",
        "**Важно**: Убедитесь, что в `backward()` подаете на вход каждому слою НЕ `input` к этому `backward`'у нейросети,\n",
        "а именно тот вход, который слой `i` получал на соответствующей итерации `forward`'а (см. `update_output`).\n",
        "То есть что вход слоя `i` $-$ это выход слоя `self.modules[i]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4qKhqmQLT9U"
      },
      "outputs": [],
      "source": [
        "class Sequential(Module):\n",
        "    \"\"\"\n",
        "        Этот класс является последовательностью модулей (слоев). \n",
        "        Последовательно обрабатывает вход `input` от слоя к слою.\n",
        "        \n",
        "        Обратите внимание, он тоже наследуется от `Module`\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__ (self):\n",
        "        super(Sequential, self).__init__()\n",
        "        self.modules = []\n",
        "   \n",
        "    def add(self, module):\n",
        "        \"\"\"\n",
        "        Добавляет модуль в контейнер.\n",
        "        \"\"\"\n",
        "        \n",
        "        self.modules.append(module)\n",
        "\n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Соответствуя разделу \"Проектирование фреймворка\":\n",
        "        \n",
        "            O_0    = module[0].forward(input)\n",
        "            O_1    = module[1].forward(O_0)\n",
        "            ...\n",
        "            output = module[n-1].forward(O_{n-2})   \n",
        "             \n",
        "        Нужно просто написать соответствующий цикл. \n",
        "        \"\"\"\n",
        "        \n",
        "        self.output = [input]\n",
        "        \n",
        "        for module in self.modules:\n",
        "          self.output.append(module.forward(self.output[-1]))\n",
        "\n",
        "        return self.output[-1]\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Соответствуя разделу \"Проектирование фреймворка\":\n",
        "            \n",
        "            g_{n-1} = module[n-1].backward(O_{n-2}, grad_output)\n",
        "            g_{n-2} = module[n-2].backward(O_{n-3}, g_{n-1})\n",
        "            ...\n",
        "            g_1 = module[1].backward(O_0, g_2)   \n",
        "            grad_input = module[0].backward(input, g_1)\n",
        "            \n",
        "        \"\"\"\n",
        "        self.grad_input = [grad_output]\n",
        "        \n",
        "        for i in range(len(self.modules) - 1, -1, -1):\n",
        "          self.grad_input.append(self.modules[i].backward(self.output[i], self.grad_input[-1]))\n",
        "        \n",
        "        return self.grad_input[-1]\n",
        "      \n",
        "\n",
        "    def zero_grad_params(self): \n",
        "        for module in self.modules:\n",
        "            module.zero_grad_params()\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        \"\"\"\n",
        "        Собирает параметры каждого слоя в один список, получая список списков.\n",
        "        \"\"\"\n",
        "        \n",
        "        return [x.get_parameters() for x in self.modules]\n",
        "    \n",
        "    def get_grad_params(self):\n",
        "        \"\"\"\n",
        "        Собирает градиенты параметров каждого слоя в один список, получая список списков.\n",
        "        \"\"\"\n",
        "        \n",
        "        return [x.get_grad_params() for x in self.modules]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Напечатать названия слоев КРАСИВО.\n",
        "        \"\"\"\n",
        "        string = \"\".join([str(x) + '\\n' for x in self.modules])\n",
        "        return string\n",
        "    \n",
        "    def __getitem__(self,x):\n",
        "        return self.modules.__getitem__(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoqSNndiLT9V"
      },
      "source": [
        "## Слои"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2YZQwg6LT9V"
      },
      "source": [
        "### Linear (2 балла = 1 [формула] + 1 [код])\n",
        "\n",
        "Линейный слой, также известный как `Fully-Connected (FC)` или `Dense`, осуществляет линейное (афинное) преобразование.\n",
        "\n",
        "Везде ниже $N$ - размер батча, $d$ - число признаков во входном тензоре, $K$ - количество нейронов в слое.\n",
        "\n",
        "*Forward pass:*\n",
        "\n",
        "$$\n",
        "x \\in \\mathbb{R}^{N \\times d}, W \\in \\mathbb{R}^{d \\times K}, b \\in \\mathbb{R}^{1 \\times K}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Linear}(x) = x W + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Linear}(x) \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "*Backward pass (1 балл):*\n",
        "Могут помочь [эта ссылка](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf), [эта ссылка](http://cs231n.stanford.edu/vecDerivs.pdf) и [эта сслыка](https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf).\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gtLkuVBLT9V"
      },
      "outputs": [],
      "source": [
        "class Linear(Module):\n",
        "    \"\"\"\n",
        "    Слой, осуществляющий линейное преобразование\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out):\n",
        "        '''\n",
        "        Поля:\n",
        "            W - матрица весов слоя размера (n_in, n_out); \n",
        "                в данном случае n_in равно числу признаков, \n",
        "                а n_out равно количеству нейронов в слое\n",
        "            b - вектор свободных членов, по одному числу на один нейрон\n",
        "            gradW - хранит градиент матрицы весов линейного слоя\n",
        "            gradb - хранит градиент вектора свободных членов\n",
        "        '''\n",
        "        super(Linear, self).__init__()\n",
        "       \n",
        "        stdv = 1./np.sqrt(n_in)\n",
        "        self.W = np.random.uniform(-stdv, stdv, size=(n_in, n_out))\n",
        "        self.b = np.random.uniform(-stdv, stdv, size=n_out)\n",
        "        \n",
        "        self.gradW = np.zeros_like(self.W)\n",
        "        self.gradb = np.zeros_like(self.b)\n",
        "        \n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "        \"\"\"\n",
        "\n",
        "        self.output = input @ self.W + self.b\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def update_grad_input(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        \n",
        "        self.grad_input = grad_output @ self.W.T\n",
        "        \n",
        "        return self.grad_input\n",
        "    \n",
        "    def update_grad_params(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        \n",
        "        self.gradW = input.T @ grad_output\n",
        "        self.gradb = grad_output.sum(axis = 0)\n",
        "        \n",
        "        assert self.gradb.shape == self.b.shape\n",
        "    \n",
        "    def zero_grad_params(self):\n",
        "        self.gradW.fill(0)\n",
        "        self.gradb.fill(0)\n",
        "        \n",
        "    def get_parameters(self):\n",
        "        return [self.W, self.b]\n",
        "    \n",
        "    def get_grad_params(self):\n",
        "        return [self.gradW, self.gradb]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        s = self.W.shape\n",
        "        q = f'Linear {s[0]} -> {s[1]}'\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I16cFZoJLT9W"
      },
      "source": [
        "### SoftMax (2 балла = 1 [формула] + 1 [код])\n",
        "\n",
        "SoftMax слой осуществляет softmax-преобразование: \n",
        "\n",
        "$$\n",
        "\\text{SoftMax}(x)_i = \\frac{\\exp(x_i)} {\\sum_j \\exp(x_j)}\n",
        "$$\n",
        "\n",
        "*Forward pass:*\n",
        "\n",
        "Обозначим `batch_size` = $N$, `n_in` = $K$.\n",
        "\n",
        "$$\n",
        "x \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "Тогда для батча SoftMax записывается так:\n",
        "\n",
        "$$\n",
        "\\text{SoftMax}(x) = \\begin{pmatrix}\n",
        "\\frac{e^{x_{11}}} {\\sum_{j=1}^K e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j=1}^K e^{x_{1j}}} & \\dots & \\frac{e^{x_{1K}}} {\\sum_{j=1}^K e^{x_{1j}}} \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "\\frac{e^{x_{N1}}} {\\sum_{j=1}^K e^{x_{Nj}}} & \\frac{e^{x_{N2}}} {\\sum_{j=1}^K e^{x_{Nj}}} & \\dots & \\frac{e^{x_{NK}}} {\\sum_{j=1}^K e^{x_{Nj}}}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{SoftMax}(x) \\in \\mathbb{R}^{N \\times K}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3O1bA8LLT9W"
      },
      "source": [
        "*Backward pass (2 балла):*\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "Смотрите *Backward pass* для `LogSoftMax` ниже. Полный вывод также есть [здесь](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) и [здесь](http://hiroshiu.blogspot.com/2018/10/gradient-of-softmax-function.html).\n",
        "\n",
        "*Подсказка:* В коде используйте свойство: $\\text{softmax}(x) = \\text{softmax}(x - \\text{const})$. Это позволяет избежать переполнения при вычислении экспоненты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB1ZZaltLT9X"
      },
      "outputs": [],
      "source": [
        "class SoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(SoftMax, self).__init__()\n",
        "\n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "        \"\"\"\n",
        "        \n",
        "        input_clamp = input - input.max(axis=1, keepdims=True)\n",
        "        output = np.exp(input_clamp)\n",
        "        self.output = output / np.sum(output, axis=1).reshape(-1, 1)\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def update_grad_input(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        input_clamp = input - input.max(axis=1, keepdims=True)\n",
        "        output = np.exp(input_clamp)\n",
        "        output = output / np.sum(output, axis=1).reshape(-1, 1)\n",
        "\n",
        "        help_arr = output * grad_output\n",
        "        self.grad_input = help_arr - output * np.sum(help_arr, axis = 1).reshape(-1, 1)\n",
        "\n",
        "        return self.grad_input\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'SoftMax'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jw9P_bTLT9Y"
      },
      "source": [
        "### LogSoftMax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN1HSCM0LT9Y"
      },
      "source": [
        "LogSoftMax слой есть просто логарифм от softmax-преобразования: \n",
        "\n",
        "$$\n",
        "\\text{logsoftmax}(x)_i = \\log(\\text{softmax}(x))_i = x_i - \\log {\\sum_j \\exp x_j}\n",
        "$$\n",
        "\n",
        "По полной аналогии с LogSoftMax-слоем распишем forward и backward:  \n",
        "\n",
        "*Forward pass:*\n",
        "\n",
        "Обозначим `batch_size` = $N$, `n_in` = $K$.\n",
        "\n",
        "$$\n",
        "x \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "Тогда для батча LogSoftMax записывается так:\n",
        "\n",
        "$$\n",
        "\\text{LogSoftMax}(x) = \\begin{pmatrix}\n",
        "x_{11} - \\log {\\sum_{j=1}^K e^{x_j}} & x_{12} - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_{1K} - \\log {\\sum_{j=1}^K e^{x_j}} \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "x_{N1} - \\log {\\sum_{j=1}^K e^{x_j}} & x_{N2} - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_{NK} - \\log {\\sum_{j=1}^K e^{x_j}} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{LogSoftMax}(x) \\in \\mathbb{R}^{N \\times K}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xh80sUfNLT9Y"
      },
      "source": [
        "*Backward pass:*\n",
        "\n",
        "LogSoftMax не имеет параметров, но применяется ко входу поэлементно, поэтому дифференцируя выход этого слоя по входу мы получаем не градиент (=вектор производных), а якобиан (=матрицу производных). Пусть $x$ сейчас $-$ это **один вектор-строка из батча**, имеющая длину $K$. \n",
        "\n",
        "#### Якобиан LogSoftMax по входу:\n",
        "\n",
        "Помним, что:\n",
        "\n",
        "$$\n",
        "\\text{LogSoftMax}(x) = \\begin{pmatrix}\n",
        "x_1 - \\log {\\sum_{j=1}^K e^{x_j}} & x_2 - \\log {\\sum_{j=1}^K e^{x_j}} & \\dots & x_K - \\log {\\sum_{j=1}^K e^{x_j}}\n",
        "\\end{pmatrix} = \\begin{pmatrix}\n",
        "b_1 & b_2 & \\dots & b_K\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$-$ обозначали за $b$ для удобства. Тогда:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\text{LogSoftMax}}{\\partial x} = \\begin{pmatrix}\n",
        "\\frac{\\partial b_1}{\\partial x_1} & \\frac{\\partial b_1}{\\partial x_2} & \\dots & \\frac{\\partial b_1}{\\partial x_K} \\\\\n",
        "\\frac{\\partial b_2}{\\partial x_1} & \\frac{\\partial b_2}{\\partial x_2} & \\dots & \\frac{\\partial b_2}{\\partial x_K} \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "\\frac{\\partial b_K}{\\partial x_1} & \\frac{\\partial b_K}{\\partial x_2} & \\dots & \\frac{\\partial b_K}{\\partial x_K} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Распишем один элемент этой матрицы и поймем, какой конкретно вид он имеет. Возьмем частную производную $b_k$ по $x_s$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial b_k}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_s} \\tag{1}\n",
        "$$\n",
        "\n",
        "Далее все зависит от $k$ и $s$. Если $k = s$, то первое слагаемое в (1) не зануляется и мы получаем:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial b_k}{\\partial x_k} = \\frac{\\partial x_k}{\\partial x_k} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = 1 - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_k} = 1 - a_k\n",
        "$$\n",
        "\n",
        "**Где $a_k$ $-$ это $k$-ая компонента SoftMax-слоя от этого входа**. Если же $k \\ne s$, то первое слагаемое в (1) обнулится:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial b_k}{\\partial x_s} = \\frac{\\partial x_k}{\\partial x_s} - \\frac{\\partial \\log {\\sum_{j=1}^K e^{x_j}}}{\\partial x_s} = - \\frac{1}{\\sum_{j=1}^K e^{x_j}} e^{x_s} = a_s\n",
        "$$\n",
        "\n",
        "Таким образом для **одной строки в батче** получаем:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\text{LogSoftMax}}{\\partial x} = \\begin{pmatrix}\n",
        "(1 - a_1) & -a_2 & \\dots & -a_K \\\\\n",
        "-a_1 & (1 - a_2) & \\dots & -a_K \\\\\n",
        "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
        "-a_1 & -a_2 & \\dots & (1 - a_K) \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "#### Вывод `grad_input`:\n",
        "\n",
        "Полностью аналогично расписанному выше для SoftMax:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_s} = \\sum_{i=1}^K \\frac{\\partial L}{\\partial b_i}\\frac{\\partial b_i}{\\partial x_s} = \\frac{\\partial L}{\\partial b_s} \\frac{\\partial b_s}{\\partial x_s} + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i}\\frac{\\partial b_i}{\\partial x_s} = \\frac{\\partial L}{\\partial b_s} (1 - a_s) + \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i} (-a_s) = \n",
        "\\frac{\\partial L}{\\partial b_s} - a_s \\frac{\\partial L}{\\partial b_s} - a_s \\sum_{i\\ne s} \\frac{\\partial L}{\\partial b_i} = \\frac{\\partial L}{\\partial b_s} - a_s \\sum_{i=1}^K \\frac{\\partial L}{\\partial b_i}\n",
        "$$\n",
        "\n",
        "Теперь легко записать формулу для `grad_input` в матричной форме, что и есть выход метода `update_grad_input()`.\n",
        "\n",
        "*Подсказка:* В коде используйте свойство: $\\text{logsoftmax}(x) = \\text{logsoftmax}(x - \\text{const})$. Это позволяет избежать переполнения при вычислении экспоненты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rwNWM9UNLT9Z"
      },
      "outputs": [],
      "source": [
        "class LogSoftMax(Module):\n",
        "    def __init__(self):\n",
        "         super(LogSoftMax, self).__init__()\n",
        "    \n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "        \"\"\"\n",
        "        # нормализуем для численной устойчивости\n",
        "        self.output = input - input.max(axis=1, keepdims=True)\n",
        "        self.output = self.output - np.log(np.sum(np.exp(self.output), axis=1)).reshape(-1, 1)\n",
        "        return self.output \n",
        "    \n",
        "    def update_grad_input(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        input_clamp = input - input.max(axis=1, keepdims=True)\n",
        "        output = np.exp(input_clamp)\n",
        "        output = output / np.sum(output, axis=1).reshape(-1, 1)\n",
        "\n",
        "        self.grad_input = grad_output\n",
        "        self.grad_input -= output * np.sum(grad_output, axis=1).reshape(-1, 1)\n",
        "        \n",
        "        return self.grad_input\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'LogSoftMax'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du0Crhg-LT9c"
      },
      "source": [
        "## Функции активации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga5mNgXdLT9d"
      },
      "source": [
        "Функции активации $-$ это нелинейные функции, которые ставятся после `Linear`, `Conv` и других слоев. Именно благодаря им нейросети являются не просто одним большим линейным преобразованием, а сложной нелинейной функцией.\n",
        "\n",
        "Достаточно исчерпывающий список с описанием преимуществ и недостатков каждой из функций активации [см. здесь](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pBDuhNULT9d"
      },
      "source": [
        "### ReLU (1 балл = 0.5 [формула] + 0.5 [код])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TJvlgs0LT9d"
      },
      "source": [
        "**[Rectified Linear Unit](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)** (**ReLU**) $-$ одна из самых часто используемых функций активации.\n",
        "\n",
        "*Forward pass:*\n",
        "\n",
        "Применяется поэлементно.\n",
        "\n",
        "$$\n",
        "x \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = max(0, x) = \\begin{cases}\n",
        "  0, & x \\le 0 \\\\    \n",
        "  x & x \\gt 0    \n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "*Backward pass:*\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tUW3UdHLT9d"
      },
      "outputs": [],
      "source": [
        "class ReLU(Module):\n",
        "    def __init__(self):\n",
        "         super(ReLU, self).__init__()\n",
        "    \n",
        "    def update_output(self, input):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "        \"\"\"\n",
        "        self.output = input\n",
        "        self.output[self.output <= 0] = 0\n",
        "        \n",
        "        return self.output\n",
        "    \n",
        "    def update_grad_input(self, input, grad_output):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя\n",
        "            `grad_output (np.array)` -- градиент по выходу этого слоя, пришедший от следующего слоя\n",
        "        \"\"\"\n",
        "        input_copy = input.copy()\n",
        "        input_copy[input_copy > 0] = 1\n",
        "        self.grad_input = grad_output * input_copy\n",
        "        \n",
        "        return self.grad_input\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'ReLU'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UVwpm8yLT9g"
      },
      "source": [
        "## Функции потерь (лосс, loss, criterion, objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK-0bBmFLT9g"
      },
      "source": [
        "*Примчание:* Формально это не функции потерь, а функции риска. Везде далее и в во всех наших материалах, связанными с нейросетями, следующие слова являются синонимами: \"лосс\", \"функция потерь\", \"loss\", \"criterion\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izE2BM5tLT9g"
      },
      "source": [
        "Функции потерь или лоссы (не путать с [мемом \"Loss\"](https://tjournal.ru/internet/68665-mem-loss)) являютя оптимизируемыми функциями в обучении с учителем. Если считать всю нейросеть одной большой функцией, то функцию потерь можно считать [функционалом](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%BE%D0%BD%D0%B0%D0%BB). \n",
        "\n",
        "Функции потерь не имеют параметров, а лишь вычисляют меру схожести ответов нейросети $\\widehat{y}$ (prediction) с истинными ответами $y$ (target, ground truth)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRHXbbhyLT9g"
      },
      "source": [
        "### Criterion\n",
        "\n",
        "**Criterion** $-$ абстрактный класс функции потерь. Этот класс можно в целом считать последним слоем нейросети, однако для удобства этот класс не является наследником `Module`, а порождает автономное семейство классов. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeNMu0dULT9g"
      },
      "outputs": [],
      "source": [
        "class Criterion(object):\n",
        "    def __init__ (self):\n",
        "        self.output = None\n",
        "        self.grad_input = None\n",
        "        \n",
        "    def forward(self, input, target):\n",
        "        \"\"\"\n",
        "        Вычисляет функцию потерь по входу `input` и истинными значениями `target`.\n",
        "\n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        Выход: \n",
        "            `self.update_output(input, target) (np.array)` -- вычисленная функция потерь\n",
        "        \"\"\"\n",
        "        return self.update_output(input, target)\n",
        "\n",
        "    def backward(self, input, target):\n",
        "        \"\"\"\n",
        "        Вычисляет градиент функции потерь по входу `input`.\n",
        "        Использует для этого также истинные значения `target`.\n",
        "\n",
        "        Вход: \n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        Выход: \n",
        "            `self.update_grad_input(input, target) (np.array)` -- вычисленный градиент по входу `input`\n",
        "        \"\"\"\n",
        "        return self.update_grad_input(input, target)\n",
        "    \n",
        "    def update_output(self, input, target):\n",
        "        \"\"\"\n",
        "        Фунция, реализующая `forward()`\n",
        "        \"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def update_grad_input(self, input, target):\n",
        "        \"\"\"\n",
        "        Фунция, реализующая `backward()`\n",
        "        \"\"\"\n",
        "        return self.grad_input   \n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Напечатать название слоя КРАСИВО.\n",
        "        \"\"\"\n",
        "        return 'Criterion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnhPXvwtLT9i"
      },
      "source": [
        "### Negative LogLikelihood criterion (численно неустойчивый) (1 балл = 0.5 [формула] + 0.5 [код])\n",
        "\n",
        "**[NLLCriterion](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss)** $-$ является отрицанием логарифма функции правдоподобия (likelihood function), используется в задаче классификации. Является частным случаем [дивергенции Кульбака-Лейблера](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D1%81%D1%82%D0%BE%D1%8F%D0%BD%D0%B8%D0%B5_%D0%9A%D1%83%D0%BB%D1%8C%D0%B1%D0%B0%D0%BA%D0%B0_%E2%80%94_%D0%9B%D0%B5%D0%B9%D0%B1%D0%BB%D0%B5%D1%80%D0%B0). Принимает на вход истинные вероятности классов $y$ и предсказанные вероятности классов $\\widehat{y}$ от `SoftMax`-слоя.\n",
        "\n",
        "Истинные метки `y` на вход ожидаются уже **после One-Hot кодирования**.\n",
        "\n",
        "*Forward pass:*\n",
        "\n",
        "$$\n",
        "\\widehat{y} \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "$$\n",
        "y \\in \\mathbb{R}^{N \\times K}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{NLLCriterion}(\\widehat{y}, y) = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\widehat{y}_{ij})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{NLLCriterion}(\\widehat{y}, y) \\in \\mathbb{R}\n",
        "$$\n",
        "\n",
        "*Backward pass (0.5 балла):*\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs1G0vB7LT9i"
      },
      "outputs": [],
      "source": [
        "class NLLCriterionUnstable(Criterion):\n",
        "    EPS = 1e-15\n",
        "    \n",
        "    def __init__(self):\n",
        "        a = super(NLLCriterionUnstable, self)\n",
        "        super(NLLCriterionUnstable, self).__init__()\n",
        "        \n",
        "    def update_output(self, input, target): \n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        \"\"\"\n",
        "        # Используйте этот трюк для избежания вычилительных ошибок\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "        \n",
        "        self.output = -((target * np.log(input_clamp)).sum())/target.shape[0]\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def update_grad_input(self, input, target):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        \"\"\"\n",
        "        # Используйте этот трюк для избежания вычилительных ошибок\n",
        "        input_clamp = np.clip(input, self.EPS, 1 - self.EPS)\n",
        "\n",
        "        self.grad_input = -target / (target.shape[0] *input_clamp)\n",
        "        \n",
        "        return self.grad_input\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'NLLCriterionUnstable'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHHeHhwSLT9i"
      },
      "source": [
        "### Negative LogLikelihood criterion (численно устойчивый) (1 балл = 0.5 [формула] + 0.5 [код])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1alK0u1LT9i"
      },
      "source": [
        "Абсолютная копия `NLLCriterionUnstable` выше, но принимает на вход не `SoftMax`-вероятности, а выход `LogSoftMax` слоя. Подобная комбинация позволяет избежать проблем в этом слое с вычислениями `forward` и `backward` для логарифма.  \n",
        "\n",
        "В коде изменения относительно `NLLCriterionUnstable` есть только в `update_grad_input()`.\n",
        "\n",
        "*Backward pass (0.5 балла):*\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp5Hb6TMLT9i"
      },
      "outputs": [],
      "source": [
        "class NLLCriterion(Criterion):\n",
        "    def __init__(self):\n",
        "        a = super(NLLCriterion, self)\n",
        "        super(NLLCriterion, self).__init__()\n",
        "        \n",
        "    def update_output(self, input, target):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        \"\"\"\n",
        "        \n",
        "        self.output = -((target *input).sum())/target.shape[0]\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def update_grad_input(self, input, target):\n",
        "        \"\"\"\n",
        "        Вход:\n",
        "            `input (np.array)` -- вход слоя  \n",
        "            `target (np.array)` -- истинные ответы\n",
        "        \"\"\"\n",
        "        \n",
        "        self.grad_input = -(target)/(target.shape[0])\n",
        "         \n",
        "        return self.grad_input\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return 'NLLCriterion'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No1H2UY-LT9j"
      },
      "source": [
        "## Оптимизаторы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtr9ORY-LT9j"
      },
      "source": [
        "В данном случае это лишь один метод оптимизации $-$ стохастический градиентный спуск (SGD)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BcAJBx8LT9j"
      },
      "source": [
        "### SGD (1 балл = 1 [код])\n",
        "\n",
        "Оптимизатор, основанный на методе стохастического градиентного спуска с `momentum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CpfEzxDLT9j"
      },
      "outputs": [],
      "source": [
        "def SGD(variables, gradients, config, state):  \n",
        "    '''\n",
        "    Реализация метода стохастического градиентого спуска с momentum.\n",
        "    Обновляет значения переменных в соответствии с их градиентами и сохраняет градиенты в state.\n",
        "    \n",
        "    Вход:\n",
        "        `variables` - список (`list`) списков переменных, которые нужно обновить \n",
        "         (один список для одного слоя)\n",
        "        `gradients` - список (`list`) списков градиентов этих переменных \n",
        "         (ровно та же структура, как и у `variables`, один список для одного слоя)\n",
        "        `config` - словарь (`dict`) c гиперпараметрами оптимизатора \n",
        "         (сейчас это только `learning_rate`)\n",
        "        `state` -  словарь (`dict`) c состоянием (`state`) оптимизатора \n",
        "         (нужен, чтобы сохранять старые значения градиентов для `momentum`)\n",
        "    Выход:\n",
        "        Ничего не возвращает. Обновляет значения градиентов \n",
        "    '''\n",
        "    \n",
        "    n = len(variables)\n",
        "    for i in range(n):\n",
        "      m = len(variables[i])\n",
        "      for j in range(m):\n",
        "        variables[i][j] -= gradients[i][j] * config['learning_rate']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIpCFWc5LT9k"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR53UZ-dLT9k"
      },
      "source": [
        "- Если хочется ускорить вычисления и написать действительно \"свой PyTorch\", можно использовать библиотеку [JAX](https://github.com/google/jax) от Google. Она является оберткой над [autograd](https://github.com/hips/autograd) (автоматическое дифференцирование) и [XLA](https://www.tensorflow.org/xla) (компиляция Python-кода)\n",
        "\n",
        "- До сих пор мы производили все вычисления на CPU. Однако Deep Learning расцвел благодаря GPU. Более конкретно $-$ благодаря [Nvidia GPU](https://developer.nvidia.com/cuda-gpus) и [Nvidia CUDA](https://developer.nvidia.com/cuda-zone). Они также очень активно используются для [компьютерной графики](https://en.wikipedia.org/wiki/Computer_graphics)\n",
        "\n",
        "- NumPy как раз можно запускать на GPU: раньше для этого чаще использовали [Numba](https://github.com/numba/numba), однако сейчас (в 2020 году) есть [много удобных библиотек для этого](https://stsievert.com/blog/2016/07/01/numpy-gpu/)\n",
        "\n",
        "- Конечно же, вы всегда можете просто использовать PyTorch для работы с GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BPXq0XfLT9k"
      },
      "source": [
        "<img src=\"https://pwnews.net/_fr/108/8422266.png\" width=500>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "[1]_task8_modules.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}